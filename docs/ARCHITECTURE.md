# ğŸ—ï¸ KIáº¾N TRÃšC REALTIME PIPELINE

TÃ i liá»‡u giáº£i thÃ­ch chi tiáº¿t kiáº¿n trÃºc, luá»“ng dá»¯ liá»‡u, vÃ  cÃ¡c thÃ nh pháº§n trong pipeline.

---

## ğŸ“Š Tá»•ng Quan Kiáº¿n TrÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    E-COMMERCE REALTIME PIPELINE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generator  â”‚â”€â”€â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â”€â”€â–¶â”‚    Spark    â”‚â”€â”€â”€â”€â”€â–¶â”‚ PostgreSQL  â”‚
â”‚  (Python)   â”‚ JSON â”‚   Topic     â”‚Streamâ”‚  Streaming  â”‚ JDBC â”‚  Database   â”‚
â”‚             â”‚      â”‚ events_raw  â”‚      â”‚   (Local)   â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                           â”‚                      â”‚
      â”‚ 5 events/sec                              â”‚ UC03-UC06            â”‚
      â”‚ Order, Payment                            â”‚ Validate, KPI        â”‚ 2 tables
      â”‚                                           â”‚                      â”‚
                                                                         â–¼
                                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                  â”‚   Frontend   â”‚
                                                                  â”‚ React + Vite â”‚
                                                                  â”‚ (localhost)  â”‚
                                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Luá»“ng Dá»¯ Liá»‡u Chi Tiáº¿t

### 1ï¸âƒ£ Event Generation

**Component**: `backend/generator.py`

```python
Event {
  id: UUID
  eventTime: ISO-8601 UTC
  eventType: order_created | payment_initiated | payment_success | payment_failed | order_cancelled
  orderId: ORD123456
  userId: USR1234
  amount: 1234567.89
  currency: VND | USD
  status: success | failed | pending
}
```

**Äáº·c Ä‘iá»ƒm**:
- Weighted random distribution (70% success, 20% pending, 10% failed)
- Configurable rate (máº·c Ä‘á»‹nh 5 events/sec)
- 1% invalid events Ä‘á»ƒ test validation
- Kafka producer vá»›i `acks=all` Ä‘áº£m báº£o delivery

---

### 2ï¸âƒ£ Kafka Topic

**Topic**: `events_raw`
- **Partitions**: 3 (cÃ³ thá»ƒ scale)
- **Replication Factor**: 1 (local setup)
- **Retention**: 24 hours
- **Format**: JSON string

**Kafka Configuration**:
```yaml
bootstrap.servers: localhost:9092
listener: PLAINTEXT
auto.create.topics: true
```

---

### 3ï¸âƒ£ Spark Structured Streaming

**Component**: `backend/spark_stream.py`

#### UC03 - Parse & Validate

```python
Read Kafka â†’ Parse JSON â†’ Validate Schema
                     â†“
          id, eventTime, eventType, orderId != null
          amount >= 0
                     â†“
          Valid Events â†’ Next stage
          Invalid Events â†’ Logged (khÃ´ng persist)
```

**Schema Enforcement**:
```python
StructType([
  StructField('id', StringType(), nullable=False),
  StructField('eventTime', StringType(), nullable=False),
  StructField('eventType', StringType(), nullable=False),
  StructField('orderId', StringType(), nullable=False),
  StructField('userId', StringType(), nullable=False),
  StructField('amount', DoubleType(), nullable=False),
  StructField('currency', StringType(), nullable=False),
  StructField('status', StringType(), nullable=False),
])
```

#### UC04 - Clean & Deduplicate

```python
Valid Events
    â†“ to_timestamp(eventTime)
Event with timestamp
    â†“ current_timestamp()
Add ingest_time
    â†“ withWatermark('event_time', '5 minutes')
Late data handling
    â†“ dropDuplicates(['id'])
Deduplicated Events â†’ events_clean
```

**Watermarking**:
- Events muá»™n hÆ¡n 5 phÃºt sáº½ bá»‹ drop
- Äáº£m báº£o consistency trong window aggregation

#### UC05 - Calculate KPIs

```python
Clean Events
    â†“ window(event_time, '1 minute')
Windowed Stream
    â†“ groupBy + agg
KPI Calculations:
  - revenue = SUM(amount) WHERE event_type = 'payment_success'
  - orders_created = COUNT WHERE event_type = 'order_created'
  - payment_success = COUNT WHERE event_type = 'payment_success'
  - payment_failed = COUNT WHERE event_type = 'payment_failed'
  - success_rate = (payment_success / total_payments) * 100
    â†“
KPI Windows (1 minute) â†’ kpi_1m
```

**Window Semantics**:
- **Window Size**: 1 minute
- **Sliding**: Non-overlapping (tumbling window)
- **Output Mode**: Update (cho aggregations)

#### UC06 - Persist to PostgreSQL

```python
2 Streaming Queries:

Query 1: events_clean
  - Output Mode: append
  - Checkpoint: ./checkpoints/spark_stream/events_clean
  - Trigger: default (as soon as possible)
  - Method: foreachBatch + JDBC

Query 2: kpi_1m
  - Output Mode: update
  - Checkpoint: ./checkpoints/spark_stream/kpi_1m
  - Trigger: default
  - Method: foreachBatch + JDBC
```

**Checkpoint**:
- Äáº£m báº£o exactly-once semantics
- Recovery khi crash
- Offset management tá»± Ä‘á»™ng

---

### 4ï¸âƒ£ PostgreSQL Database

**Database**: `realtime` | User: `app` | Password: `app`

#### Table: events_clean
```sql
CREATE TABLE events_clean (
    id VARCHAR(50) PRIMARY KEY,
    event_time TIMESTAMP NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    order_id VARCHAR(50) NOT NULL,
    user_id VARCHAR(50) NOT NULL,
    amount DECIMAL(15, 2) NOT NULL,
    currency VARCHAR(10) NOT NULL,
    status VARCHAR(20) NOT NULL,
    ingest_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Indexes
CREATE INDEX idx_events_clean_event_time ON events_clean(event_time DESC);
CREATE INDEX idx_events_clean_event_type ON events_clean(event_type);
CREATE INDEX idx_events_clean_order_id ON events_clean(order_id);
```

**Purpose**: Store all cleaned events for audit, traceability

#### Table: kpi_1m
```sql
CREATE TABLE kpi_1m (
    window_start TIMESTAMP PRIMARY KEY,
    window_end TIMESTAMP NOT NULL,
    revenue DECIMAL(18, 2) NOT NULL DEFAULT 0,
    orders_created INTEGER NOT NULL DEFAULT 0,
    payment_success INTEGER NOT NULL DEFAULT 0,
    payment_failed INTEGER NOT NULL DEFAULT 0,
    success_rate DECIMAL(5, 2) DEFAULT 0,
    processed_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Index
CREATE INDEX idx_kpi_1m_window_start ON kpi_1m(window_start DESC);
```

**Purpose**: Pre-aggregated KPIs for fast dashboard queries

#### Views: v_kpi_15m, v_kpi_1h, v_kpi_24h
```sql
CREATE VIEW v_kpi_15m AS
SELECT 
  SUM(revenue) as total_revenue,
  SUM(orders_created) as total_orders,
  ...
FROM kpi_1m
WHERE window_start >= NOW() - INTERVAL '15 minutes';
```

**Purpose**: Quick aggregation for different time ranges

---

### 5ï¸âƒ£ Frontend Dashboard

**Stack**: React 18 + Vite + TypeScript + TailwindCSS

#### Architecture
```
src/
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ dashboard/      # Business KPI page
â”‚   â”œâ”€â”€ events/         # Events table page
â”‚   â””â”€â”€ ops/            # Operations console
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/             # Reusable UI components
â”‚   â””â”€â”€ layout/         # Layout components
â””â”€â”€ lib/
    â””â”€â”€ api.ts          # API client (MOCK/REAL mode)
```

#### API Layer
```typescript
// api.ts
export const api = {
  getKpi(timeRange): Promise<BusinessKPI>
  getTimeSeries(timeRange): Promise<TimeSeriesData[]>
  getEvents(params): Promise<EventsResponse>
  getSystemHealth(): Promise<SystemHealth>
}

// Modes
USE_MOCK = true   â†’ Mock data generator
USE_MOCK = false  â†’ Real API (PostgreSQL)
```

---

## âš¡ Performance Characteristics

### Throughput
- **Generator**: 5-10 events/sec (configurable)
- **Kafka**: HÃ ng nghÃ¬n events/sec (single broker)
- **Spark**: 100-500 events/sec (local mode, 4 cores)
- **PostgreSQL**: 500+ inserts/sec

### Latency (End-to-End)
```
Event Generated â†’ Kafka â†’ Spark â†’ PostgreSQL â†’ Dashboard
     0ms           ~10ms    ~1s      ~100ms      ~10s (polling)
                                                  
Total: ~1-2 seconds tá»« event táº¡o Ä‘áº¿n database
```

### Resource Usage
- **Docker (Kafka + Postgres)**: ~2GB RAM
- **Spark Local**: ~2-4GB RAM
- **Generator**: ~50MB RAM
- **Total**: ~5GB RAM recommended

---

## ğŸ”’ Reliability & Fault Tolerance

### Exactly-Once Semantics
```
Kafka Producer (acks=all)
    â†“
Spark Checkpointing
    â†“
JDBC Transactional Write
    â†“
PostgreSQL ACID
```

### Failure Scenarios

#### 1. Generator Crash
- **Impact**: No new events
- **Recovery**: Restart generator, no data loss
- **Duration**: Immediate

#### 2. Kafka Broker Down
- **Impact**: Producer cannot send, Spark cannot read
- **Recovery**: Restart Kafka, replay from checkpoint
- **Duration**: 1-2 minutes

#### 3. Spark Job Crash
- **Impact**: No processing, events accumulate in Kafka
- **Recovery**: Restart Spark, resume from checkpoint
- **Duration**: 30 seconds - 1 minute
- **Data Loss**: None (exactly-once)

#### 4. PostgreSQL Down
- **Impact**: Spark cannot write, buffering in memory
- **Recovery**: Restart PostgreSQL, Spark retries
- **Duration**: 30 seconds
- **Data Loss**: None (checkpoint-based)

---

## ğŸ“ˆ Scalability

### Current Setup (Local)
- 1 Kafka broker
- 1 Spark local executor (4 cores)
- 1 PostgreSQL instance
- **Capacity**: ~10-50 events/sec

### Scale-Out Strategy

#### Phase 1: Vertical Scaling
- TÄƒng RAM cho Spark (4GB â†’ 8GB)
- TÄƒng PostgreSQL connection pool
- **Capacity**: ~100 events/sec

#### Phase 2: Horizontal Scaling
- 3 Kafka brokers (replication factor 3)
- Spark cluster mode (3 executors)
- PostgreSQL read replicas
- **Capacity**: ~1000 events/sec

#### Phase 3: Cloud Native
- Kafka â†’ AWS MSK / Confluent Cloud
- Spark â†’ AWS EMR / Databricks
- PostgreSQL â†’ AWS RDS / Aurora
- **Capacity**: ~10,000+ events/sec

---

## ğŸ” Security Considerations

### Current (Development)
- âŒ No authentication
- âŒ PLAINTEXT connections
- âŒ Default passwords

### Production Requirements
- âœ… Kafka SASL/SSL
- âœ… PostgreSQL SSL + strong passwords
- âœ… Spark authentication
- âœ… API authentication (JWT)
- âœ… Network segmentation

---

## ğŸ“š Technology Choices

| Component | Technology | Why? |
|-----------|-----------|------|
| **Message Queue** | Kafka | Industry standard, high throughput, replay capability |
| **Stream Processing** | Spark Structured Streaming | Unified batch/stream, SQL-like API, mature ecosystem |
| **Database** | PostgreSQL | ACID compliance, mature, great for analytics |
| **Language** | Python | Easy for students, rich libraries |
| **Frontend** | React + Vite | Fast dev experience, component-based |

---

## ğŸ“ Há»c Tá»« Kiáº¿n TrÃºc NÃ y

### Data Engineering Concepts
1. **Lambda Architecture**: Batch + Stream processing
2. **Event Sourcing**: Immutable event log
3. **CQRS**: Command (events_clean) vs Query (kpi_1m)
4. **Windowing**: Tumbling windows cho aggregation
5. **Watermarking**: Late data handling

### Best Practices
1. **Idempotency**: Duplicate detection qua unique ID
2. **Checkpointing**: Fault tolerance
3. **Schema Evolution**: Structured data vá»›i validation
4. **Monitoring**: Logs, metrics, health checks
5. **Documentation**: Comprehensive docs cho maintainability

---

**ğŸ“– Xem thÃªm**:
- [BACKEND_SETUP.md](BACKEND_SETUP.md) - Setup guide
- [QUICKSTART.md](QUICKSTART.md) - Quick start
- [../README.md](../README.md) - Project overview
